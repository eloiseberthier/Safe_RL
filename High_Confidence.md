# High Confidence Policy Improvement

*The user may select any performance lower-bound rho-, and confidence level delta, and our algorithm will ensure that the probability that it retruns a policy with performance below rho- is at most delta.*

## Introduction

Focus on ensuring *safety*: the probability that our algo returns a policy with performance below a baseline rho- is at most delta.

Batch and incremental policy improvement algorithms that provide safety guarantees for every policy they propose. No hyperparameter to tune. Drawbacks: high computational complexity, and tend to require more data than ordinary RL algos.

CPI (Kakade, 2002) and SPI (Pirotta, 2013) are the only RL algos with monotically improving behavior. CPI was not intended to be safe, and CPI requires an impractical number of trajectories to ensure safety.

## Relevant material

MDP (or POMDP). We have assumed that the initial state distribution, the machanisms producing the rewards, observations and state transitions are all stationary.

tau a trajectory, with normalized and discounted return: R(tau) = 1/(R+ - R-) (  \sum_t^T gamma^(t-1) r^tau_t   - R-).

assume all trajectories of length at most T.

rho(pi) = E[R(tau)|pi] the performance of pi.

D a set of n trajectories tau_i, each labeled by the policy that generated it pi_i (behavior policies).

Define a mixed policy mu_alpha,pi0,pi to be a mixture of pi0 and pi with mixing parameter alpha: mu_alpha,pi0,pi = alpha pi + (1-alpha) pi0.

## High-Confidence Off-Policy Evaluation (HCOPE)

cites [5] on HCOPE.

Taking a set of trajectories D generated using some behavior policies, and using them to lower-bound the performance of another policy pie called the evaluation policy.

Important (?): We can not just lower-bound the performance of a policy given some trajectories, but we can also predict from some trajectories what the lower bound would be if computer later with more trajectories.

3 different approaches, each based on importance sampling, used to produce an unbiased estimator of rho(pie) from a trajectory tau generated by a behavior policy pib. This estimator is called the importance weighted return:

rhohat(pie | tau, pib) = R(tau) w(tau, pie, pib), where w is the importance weight, w(tau, pie, pib) = \prod_t=1^T pie(a_t^tau | s_t^tau) / pib(a_t^tau | s_t^tau).

The importance wieghted returns rhohat are independent variables Xi with the same expected value, used to estimate xbar = rho(pie). 

### A concentration Inequality (CI) for HCOPE

Method in [5]. Concentration inequality approach. Can be used to compute an estimate of what the 1-delta confidence lower bound on rho(pie) *would be if computed using m trajectories rather than n*. An explicit algorithm to compute this bound is provided. Certainly more details in [5].

Benefit: introduces no false assumptions. Drawback: tends to be *overly conservative*.

Following: other approaches with typically false assumptions, but tighter lower bounds.

### Student's t-test (TT) for HCOPE

The sample mean Xhat of the Xi is also an unbiased estimator of xbar. Central Limit Theorem: Xhat approximates a normal distribution. It provides an algorithm based on one-sided Student's t-test (correct *if Xhat was normally distributed*).

It gives tighter bounds, but Xhat is not normal in general. If n is large enough, quite reasonable. However the importance weights often come from heavy upper tailed distributions, which can make the t-test *overly conservative*, even for large datasets. And in some cases, it may result in an error rate greater than gamma (but dominated by the POMDP assumption).

### A Bootstrap Confidence Interval (BCa) for HCOPE

Bias Corrected and accelerated (BCa) bootstrap. Using a bootstrap estimate of the distribution of Xhat, BCa can correct for the heavy tails to produce lower bounds *not overly conservative*. For some distributions, the error rate might be higher than delta. However, reliable enough to be used in medical research.

## Problem Formulation

Given a user select performance lower-bound rho- and confidence level delta, an RL algorithm is called *safe* if it ensures that the probability that a policy with performance below rho- will be proposed is at most delta.

Only assumption: POMDP environment. Safety must hold regardless of how hyperparameters are tuned.

An RL algorithm is called *semi-safe* if it would be safe, except that it makes a false but reasonable assumption. Interesting when the assumption that the environment is a POMDP is stronger than the other assumption (like importance weighted returns being normally distributed).

A policy pi is called *safe* if we can ensure that rho(pi)>= rho- with confidence 1-delta.

The policy improvement mechanism should return the safe policy expected to perform the best: pi' = argmax_safe pi g(pi | D), where g is a prediction of rho(pi) computed from D.

We use weighted importance sampling for g (lower variance but biased):
g(pi | D) = (\sum_i rhohat(pi | tau_i, pi_i)) / (\sum_i what(tau_i, pi, pi_i))

In this paper: batch and incremental policy improvement algorithms that are safe when they use the CI approach to HCOPE, and semi-safe with TT or BCa. No hyperparameters.

## Safe Policy Improvement (batch)

possible problem: multiple comparisons problem is when we use the same data to search the space of policies and to perform safety tests. So we set aside data only for the safety test: split Dtrain / Dtest (5/1).

Algorithm: get candidate policy, test it, if the performance is above rho- return it, else return no safe policy found.

algorithm fo get candidate policy: maximize the performance among policies predicted to pass the safety test. If none found, returns the policy with the highest lower bound. But tendency to overfit.

another algo: regularization parameter to penalize distance to the initial policy, set with kfold cross validation. Change the proba of an action no more than alpha towards being deterministic.

## Safe Policy Improvement (incremental)

Maintain a list C of policies deemed safe. Always use the policy in C expected to perform best. Generate beta trajectories for the current policy, run policy improvement, append new policy.

## Empirical tests on real problems

see the article.
